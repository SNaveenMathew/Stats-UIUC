Tree can predict in presenece of missing values by using surrogate models built based on correlation with variable that has missing values.

## Impurity:

Uncertainty in predicting outcome

### Gini (CART):

$$Gini = \sum_{i=1}^{K}p_i(1-p_i) = 1 - \sum_{i=1}^{K}p_i^2$$

If for some k, we have $p_k = 1$, then $p_i = 0\ \forall i \ne k$

$Score = Gini(A) - \frac{|A_L|}{|A|}Gini(A_L) - \frac{|A_R|}{|A|}Gini(A_R)$

Larger => More benefit by splitting

Since A does not change in a split, we have $min\Bigg(\frac{|A_L|}{|A|}Gini(A_L) + \frac{|A_R|}{|A|}Gini(A_R)\Bigg)$

Try this for all different variables and all different cut points to calculate score. Choose the variable-knot combination with largest score.

### Variance

$$Var(A) = \frac{1}{|A|}\sum_{i = 1}^{n}(x_i - \bar{x})^2$$

### Shannon entropy (ID3/C4.5):

$$Entropy(A) = -\sum_{i=1}^{K}p_ilog(p_i)$$


Similar to histogram estimator because for 'similar' examples we predict the same value (mean of training outcomes of 'similar' examples). Similar: fall in the same node of the tree

## Splitting on categorical variable:

If there are C factor levels, there are $2^{C-1}-1$ ways of splitting into 2 groups. If C > 10, the package splits into 2 subsets randomly. If C > 53, the package does not work.

## Comparing histogram estimator and CART:

CART puts more splits in areas where the function changes drastically and lesser splits in areas where the function has small curvature.

Let us assume we have an underlying function that has small curvature. Let us assume we have a histogram estimator and a CART. Benefit of splitting more in histogram estimator: reduction in bias, but at the cost of increase in variance. Since the underlying function curvature is small, reduction in bias may not cancel increase in variance. Therefore, CART splits are better.

Let us assume we have an underlying function that has large curvature. Let us assume we have a histogram estimator and a CART. Benefit of splitting more in histogram estimator: reduction in bias, but at the cost of increase in variance. CART already has smaller bias because of finer splits.

This means that CART usually has better performance (bias-variance tradeoff) than kernel methods such as histogram estimator. CART can be considered as method with adaptive kernel

## Penalized:

$$C_\alpha(T) = \sum_{all\ terminal\ nodes\ A\ of\ T}|A|.Gini(A) + \alpha |T|$$

## Bagging:

Bootstrap (sampling) aggregation: Build CART on each bootstrap sample and average the predictions (do not average the models, just average the predictions).

It is known that CART is very unstable. By performing bootstrap sampling and averaging, the variance will be reduced (each bootstrap sample is independent, therefore their model predictions will not be highly correlated)

If $\hat{f}$ is independent, $var(\frac{1}{B}\sum\hat{f}(x)) = \frac{1}{B}var(\hat{f}(x))$

## Random Forest:

Ho's method: To remove correlation, we select random subset of features to build trees

Random forest: Select only `mtry` variables for each split (different from Ho's method which uses subset of features for the whole tree). This leads to less correlation between $\hat{f}$

Node size is the (minimum) sample size at each terminal node

Main tuning parameters are node size and mtry. Ntree should be sufficiently large if p is large and mtry is small because total number of combinations of trees is very large

Prediction: $\hat{f}(x) = \frac{1}{B}\sum_{b=1}{B}\hat{f_b}(x) = \frac{1}{B}\sum_{b=1}^{B}\frac{\sum_{i=1}^{n}I(x_i \in A_b)y_i}{\sum_{i=1}^{n}I(x_i \in A_b)} = \frac{\sum_{i=1}^{n}w_iy_i}{\sum_{i=1}^{n}w_i}$

Points in same split have higher weights. If model does not split on a variable, then the kernel weight for a test example does not depend on that variable - advantageous compared to Gaussian kernel which gives equal importance to all dimensions. This way random forest reduces dimensions in kernel perspective.

### Variable importance:

Out-of-bag data: data left out while performing bootstrap sampling. This will be used as testing data for the tree - can be used to estimate test error.

From the oob set, choose a variable $X_j$ and replace the values with random noise. Compute the predictions of random forest. Calculate the change in error.

$VI_{bj} = \frac{Err_j^b}{Err_0^b} - 1, Err_j^b\text{ is oob error after randomizing }X_j; Err_0^b\text{ is oob error wihtout randomizing the predictor}$