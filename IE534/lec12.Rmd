---
title: "Lecture 12"
author: "Naveen Mathew Nathan S."
date: "10/3/2019"
output: pdf_document
---

# ResNet

- Vanishing gradient in deep convolutional networks
- Information flow from lower layers to higher layers facilitiated by shortcut connections from lower layers to higher layers

## Notation

- Input: $x \in R^{d \times d \times C_0}$
- Kernel: $K \times K \times C$

## Basic block

- x -> weight layer -> ReLU -> weight layer -> F(x)
- Output of basic block is F(x) + x: summation, not concatenation. Therfore x and F(x) have same dimension
- If output layer is connected to basic block: $y = F(x, \{W_i\}) + x$

## Bottleneck block

- 1 x 1, 3 x 3 and 1 x 1 convolutions: 1 x 1 layers reduce and then increase dimensions, 3 x 3 layer is bottleneck with smaller input/output dimensions

## Dimension mismatch between F(x) and x

- Spatial matching
    - Use linear projection $y = F(x, \{W_i\}) + W_sx$
    - Some operations with stride
- Channel mismatch
    - 1 x 1 convolution
    - Input: W x H x $C_1$
    - Kernel: 1 x 1 x $C_2$
    - Output: W x H x $C_2$

## Distributed training

- MPI supports only CPU communication. Therefore, model and tensors should be sent to CPU before communication

## Communication between nodes

- Point-to-point: transfer data from one process to another; achieved using send-recv, isend-irecv
    - Synchronous
    - Asynchronous
- Collective
    - Synchronous

## Blocking vs unblocking

- Blocking: can't process until previous process completes. Uses send
- Unblocking: Uses isend

## Collective communication

- Scatter Designated root process sends data to all processes in communicator
- Broadcast: send tensor to all nodes
- Gather: Take elements from all other nodes to one node
- All-gather: Take elements of all nodes to all other nodes
- Reduce: similar to gather
- All-reduce: Every node gets sum of all gradient
